Perfect â€” I will now update the full project plan to use Supabase + React, instead of Node.js + Express + MongoDB.

This version is fully optimized for a Hackathon, faster to build, and uses Supabase as:

âœ” Authentication
âœ” Database (PostgreSQL)
âœ” Storage (Resume upload PDFs)
âœ” Edge Functions (AI, scraping, parsing)
âœ” Row Level Security (RLS)
âœ” API endpoints replacement

ğŸš€ UPDATED PROJECT PLAN (Supabase + React Version)

Your new stack:

Frontend: React + Vite + Tailwind + React Router + Axios/Fetch

Backend: Supabase (Auth + DB + Storage + Edge Functions)

AI: Supabase Edge Functions (Deno runtime) + OpenAI API

Scraping: Edge Functions using Cheerio + Fetch

Resume Parsing: Edge Functions (extract text + AI processing)

âœ… SUPABASE DATABASE SCHEMA (PostgreSQL)

You must create these tables inside Supabase.

1. users (auto managed by Supabase Auth)

Supabase automatically manages:

id (UUID)

email

created_at

We extend it with a public.users table for profile data.

2. profiles table
create table profiles (
  id uuid primary key references auth.users(id) on delete cascade,
  name text,
  avatar_url text,

  resume_url text,
  resume_raw_text text,

  skills text[],
  experience jsonb,
  education jsonb,
  projects jsonb,

  created_at timestamptz default now()
);

3. job_matches table
create table job_matches (
  id uuid primary key default gen_random_uuid(),
  user_id uuid references profiles(id) on delete cascade,

  title text,
  company text,
  link text,
  score int,
  reasons text,
  requirements text[],
  date_found timestamptz default now()
);

4. interview_history table
create table interview_history (
  id uuid primary key default gen_random_uuid(),
  user_id uuid references profiles(id) on delete cascade,

  company text,
  role text,
  tech_stack text[],
  date timestamptz default now(),

  questions jsonb
);

5. job_cache (optional but useful)
create table job_cache (
  id uuid primary key default gen_random_uuid(),
  query text,
  results jsonb,
  timestamp timestamptz default now()
);

ğŸ” RLS Policies

Enable RLS for:

profiles

job_matches

interview_history

Then add policies:

-- User can select their own profile
create policy "Users can select own data" 
on profiles for select 
using (auth.uid() = id);

-- User can update own data
create policy "Users can update own data"
on profiles for update
using (auth.uid() = id);


Do similar for:

job_matches

interview_history

âš¡ SUPABASE EDGE FUNCTIONS YOU MUST CREATE

You will use Edge Functions as your backend API.

1. parse_resume function

Purpose:

Accept PDF upload URL

Extract text

AI analyze resume

Store parsed data in DB

Flow:

upload â†’ storage url â†’ Edge Function â†’ pdf â†’ AI extraction â†’ save in profiles


Function tasks:

Fetch the PDF from Supabase storage

Extract text (PDF.js or a Deno library)

Send text to OpenAI for structured JSON

Upsert into profiles table

2. discover_jobs function

Purpose:

Read user profile (skills)

Scrape jobs using scraping portals

Ask AI to score jobs

Save in job_matches

Flow:

skills â†’ scrape â†’ AI scoring â†’ store in DB â†’ return results

3. interview_generator function

Purpose:

Input: company, role, tech stack

Scrape company data

Generate deep technical + behavioral questions

Store in interview_history

Flow:

input â†’ scrape â†’ AI generate â†’ save â†’ return

ğŸ¨ FRONTEND (REACT) PLAN

Pages remain mostly the same but backend calls change â†’ now calling Supabase + Edge Functions.

ğŸŒ FRONTEND PAGES
1. Auth Pages

Using supabase.auth.signUp() and signInWithPassword().

/login
/register

Store session via Supabase Auth Helper.

2. Upload Resume Page
/upload-resume

Upload PDF to Supabase Storage

Call parse_resume function

Show parsed results

3. Job Discovery Page
/jobs

Call Supabase Edge Function:

POST /discover_jobs


Display:

title

company

match score

reason

4. Interview Prep Page
/interview

Form Inputs:

company

role

tech stack

Call:

POST /interview_generator


Display:

questions

answers

explanations

5. Dashboard Page
/dashboard

Fetch from DB:

skills

job_matches

interview_history

insights (computed client-side OR via function)

Use Recharts for graphs:

Pie chart â†’ skill distribution

Bar chart â†’ job scores

Timeline â†’ interview history

ğŸ—‚ï¸ FRONTEND FOLDER STRUCTURE (React)
src/
 â”œâ”€ components/
 â”œâ”€ pages/
 â”‚   â”œâ”€ Auth/
 â”‚   â”œâ”€ UploadResume.jsx
 â”‚   â”œâ”€ JobDiscovery.jsx
 â”‚   â”œâ”€ InterviewPrep.jsx
 â”‚   â”œâ”€ Dashboard.jsx
 â”œâ”€ supabase/
 â”‚   â”œâ”€ client.js  (supabase config)
 â”œâ”€ api/
 â”‚   â”œâ”€ resume.js
 â”‚   â”œâ”€ jobs.js
 â”‚   â”œâ”€ interview.js
 â”œâ”€ App.jsx
 â”œâ”€ main.jsx

ğŸŒ©ï¸ SUPABASE EDGE FUNCTIONS STRUCTURE
supabase/
 â”œâ”€ functions/
 â”‚   â”œâ”€ parse_resume/
 â”‚   â”‚    â”œâ”€ index.ts
 â”‚   â”œâ”€ discover_jobs/
 â”‚   â”‚    â”œâ”€ index.ts
 â”‚   â”œâ”€ interview_generator/
 â”‚   â”‚    â”œâ”€ index.ts

ğŸ—“ï¸ HACKATHON IMPLEMENTATION TIMELINE
Day 1 â€“ Supabase Setup

Create tables

Setup auth

Connect React to Supabase

Build Login + Register

Day 2 â€“ Resume Upload + Parsing

Storage + file upload

parse_resume function

Show parsed skills

Day 3 â€“ Job Discovery

discover_jobs function

Build frontend job list

Store job matches

Day 4 â€“ Interview Preparation

interview_generator function

Display question bank

Save history

Day 5 â€“ Dashboard + Final Cleanup

Charts

Insights

Polish UI

Remove bugs

Connect all flows

âœ¨ NEXT STEP (Tell me one word):

â€œGenerate Supabase schema SQLâ€
or
â€œGenerate Edge Functions codeâ€
or
â€œGenerate React pagesâ€
or
â€œGive me the full system folder structure with codeâ€